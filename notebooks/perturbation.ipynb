{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniellee/Library/Caches/pypoetry/virtualenvs/refusal-direction--eHOUtxP-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from sae_vis import SaeVisConfig, SaeVisData, SaeVisLayoutConfig\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "device = \"cuda\" if t.cuda.is_available() else \"mps\" if t.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available.\n",
      "Memory information is not available for MPS.\n"
     ]
    }
   ],
   "source": [
    "## check memory usage\n",
    "\n",
    "if t.cuda.is_available():\n",
    "    gpu_id = 0  # Set to your target GPU ID\n",
    "    total_memory = t.cuda.get_device_properties(gpu_id).total_memory\n",
    "    allocated_memory = t.cuda.memory_allocated(gpu_id)\n",
    "    cached_memory = t.cuda.memory_reserved(gpu_id)\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory / 1024**2:.2f} MB\")\n",
    "    print(f\"Allocated GPU Memory: {allocated_memory / 1024**2:.2f} MB\")\n",
    "    print(f\"Cached GPU Memory: {cached_memory / 1024**2:.2f} MB\")\n",
    "elif t.backends.mps.is_available():\n",
    "    # MPS (Metal Performance Shaders) for Mac\n",
    "    print(\"MPS is available.\")\n",
    "    # Note: As of now, PyTorch doesn't provide direct memory management functions for MPS\n",
    "    print(\"Memory information is not available for MPS.\")\n",
    "else:\n",
    "    print(\"Neither CUDA nor MPS is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.84s/it]\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b-it into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gemma2: HookedSAETransformer = HookedSAETransformer.from_pretrained(\"gemma-2-2b-it\", device=device)\n",
    "\n",
    "layer = 5\n",
    "sae_name = \"gemma-scope-2b-pt-res-canonical\"\n",
    "sae_id = f\"layer_{layer}/width_16k/canonical\"\n",
    "\n",
    "gemma2_sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "            release=sae_name,\n",
    "            sae_id=sae_id,\n",
    "            device=str(device),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basically make the steering coefficient negative of the activaiton\n",
    "\n",
    "def generate_with_steering(\n",
    "    model: HookedSAETransformer,\n",
    "    sae: SAE,\n",
    "    prompt: str,\n",
    "    latent_idx: int,\n",
    "    steering_coefficient: float = 1.0,\n",
    "    max_new_tokens: int = 50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates text with steering. A multiple of the steering vector (the decoder weight for this latent) is added to\n",
    "    the last sequence position before every forward pass.\n",
    "    \"\"\"\n",
    "    _steering_hook = partial(\n",
    "        steering_hook,\n",
    "        sae=sae,\n",
    "        latent_idx=latent_idx,\n",
    "        steering_coefficient=steering_coefficient,\n",
    "    )\n",
    "\n",
    "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, _steering_hook)]):\n",
    "        output = model.generate(prompt, max_new_tokens=max_new_tokens, **GENERATE_KWARGS)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "prompt = \"When I look at myself in the mirror, I see\"\n",
    "\n",
    "no_steering_output = gemma_2_2b.generate(prompt, max_new_tokens=50, **GENERATE_KWARGS)\n",
    "\n",
    "table = Table(show_header=False, show_lines=True, title=\"Steering Output\")\n",
    "table.add_row(\"Normal\", no_steering_output)\n",
    "for i in tqdm(range(3), \"Generating steered examples...\"):\n",
    "    table.add_row(\n",
    "        f\"Steered #{i}\",\n",
    "        generate_with_steering(\n",
    "            gemma_2_2b,\n",
    "            gemma_2_2b_sae,\n",
    "            prompt,\n",
    "            latent_idx,\n",
    "            steering_coefficient=240.0,  # roughly 1.5-2x the latent's max activation\n",
    "        ).replace(\"\\n\", \"↵\"),\n",
    "    )\n",
    "rprint(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
