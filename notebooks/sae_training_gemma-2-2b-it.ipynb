{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "395d84f5-2a27-4399-8978-44abbe2e80f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/data2/hms/dbmi/sunyaev/lab/dlee/.cache/pypoetry/virtualenvs/refusal-direction-f5Ymycjl-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dataclasses import dataclass, fields\n",
    "\n",
    "import torch as t\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ff19ac-19ac-4ae3-a1dd-637d9b7b2938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd77fce2-0937-40ae-90eb-1de64ce3d8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 81037.75 MB\n",
      "Allocated GPU Memory: 0.00 MB\n",
      "Cached GPU Memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "if t.cuda.is_available():\n",
    "    gpu_id = 0  # Set to your target GPU ID\n",
    "    total_memory = t.cuda.get_device_properties(gpu_id).total_memory\n",
    "    allocated_memory = t.cuda.memory_allocated(gpu_id)\n",
    "    cached_memory = t.cuda.memory_reserved(gpu_id)\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory / 1024**2:.2f} MB\")\n",
    "    print(f\"Allocated GPU Memory: {allocated_memory / 1024**2:.2f} MB\")\n",
    "    print(f\"Cached GPU Memory: {cached_memory / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb7e84-f90e-4b5c-b0bf-c59b74cc9afa",
   "metadata": {},
   "source": [
    "export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6442d69a-6dc1-4329-ab1e-b352b8877898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a large random tensor on the GPU\n",
    "# tensor = t.randn(10000, 10000, device=device)\n",
    "# del tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f56df1-ab67-4442-9a4f-54a138223de3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sae_gemma_2b_it, cfg_gemma_2b_it, sparsity_gemma_2b_it  = SAE.from_pretrained(\n",
    "  \"gemma-2b-it-res-jb\", # to see the list of available releases, go to: https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n",
    "  \"blocks.12.hook_resid_post\" # change this to another specific SAE ID in the release if desired. \n",
    ")\n",
    "\n",
    "del sae_gemma_2b_it, sparsity_gemma_2b_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a28f257-14f7-416a-bd83-0b6e926442f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_gemma_scope, cfg_gemma_scope, sparsity_gemma_scope  = SAE.from_pretrained(\n",
    "  release = \"gemma-scope-2b-pt-res-canonical\", # to see the list of available releases, go to: https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n",
    "  sae_id = \"layer_5/width_16k/canonical\" # change this to another specific SAE ID in the release if desired. \n",
    ")\n",
    "\n",
    "del sae_gemma_scope, sparsity_gemma_scope\n",
    "\n",
    "t.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c494c3-f0dd-4c9f-8dbf-d389e380101a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 16384-L1-2-LR-5e-05-Tokens-1.229e+09\n",
      "n_tokens_per_buffer (millions): 0.131072\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.000128\n",
      "Total training steps: 300000\n",
      "Total wandb updates: 6000\n",
      "n_tokens_per_feature_sampling_window (millions): 20971.52\n",
      "n_tokens_per_dead_feature_window (millions): 20971.52\n",
      "We will reset the sparsity calculation 60 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+07\n"
     ]
    }
   ],
   "source": [
    "# Function to load the JSON configuration into the dataclass\n",
    "\n",
    "def load_config(config_path: str) -> LanguageModelSAERunnerConfig:\n",
    "    with open(config_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Ensure no extra fields are present\n",
    "    valid_fields = {f.name for f in fields(LanguageModelSAERunnerConfig)}\n",
    "\n",
    "    # Filter the data to only include valid fields\n",
    "    filtered_data = {k: v for k, v in data.items() if k in valid_fields}\n",
    "\n",
    "    if \"expansion_factor\" in filtered_data.keys() and \"d_sae\" in filtered_data.keys():\n",
    "        filtered_data = {k: v for k, v in filtered_data.items() if k != 'expansion_factor'}\n",
    "    \n",
    "    return LanguageModelSAERunnerConfig(**filtered_data)  # Unpack the JSON dictionary into the dataclass\n",
    "\n",
    "# Usage example\n",
    "cfg = load_config('gemma_2b_it_blocks.12.hook_resid_post_16384_cfg.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d736cd5a-87e2-4fcd-adb5-8fba6d6ba1e3",
   "metadata": {},
   "source": [
    "some things to change from jbloom/Gemma-2b-IT-Residual-Stream-SAEs:\n",
    "- change model to gemma-2-2b-it\n",
    "- change layer\n",
    "- change architecture from standard --> gated or jumprelu\n",
    "- apply_b_dec_to_input --> depending on the SAE method\n",
    "- maybe: dataset\n",
    "- make training longer\n",
    "\n",
    "parameters to play around:\n",
    "- l1 coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "432c7695-5dbf-4bc2-b33b-406dbe3d197e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModelSAERunnerConfig(model_name='gemma-2b-it', model_class_name='HookedTransformer', hook_name='blocks.12.hook_resid_post', hook_eval='NOT_IN_USE', hook_layer=12, hook_head_index=None, dataset_path='chanind/openwebtext-gemma', dataset_trust_remote_code=True, streaming=False, is_dataset_tokenized=True, context_size=1024, use_cached_activations=False, cached_activations_path=None, architecture='standard', d_in=2048, d_sae=16384, b_dec_init_method='zeros', expansion_factor=None, activation_fn='relu', activation_fn_kwargs={}, normalize_sae_decoder=False, noise_scale=0.0, from_pretrained_path=None, apply_b_dec_to_input=False, decoder_orthogonal_init=False, decoder_heuristic_init=True, init_encoder_as_decoder_transpose=True, n_batches_in_buffer=16, training_tokens=1228800000, finetuning_tokens=0, store_batch_size_prompts=8, train_batch_size_tokens=4096, normalize_activations='none', seqpos_slice=(None,), device='cuda', act_store_device='cuda', seed=42, dtype='float32', prepend_bos=True, autocast=False, autocast_lm=False, compile_llm=True, llm_compilation_mode='max-autotune', compile_sae=False, sae_compilation_mode=None, adam_beta1=0.9, adam_beta2=0.999, mse_loss_normalization=None, l1_coefficient=2, lp_norm=1.0, scale_sparsity_penalty_by_decoder_norm=True, l1_warm_up_steps=15000, lr=5e-05, lr_scheduler_name='constant', lr_warm_up_steps=0, lr_end=5e-06, lr_decay_steps=60000, n_restart_cycles=1, finetuning_method=None, use_ghost_grads=False, feature_sampling_window=5000, dead_feature_window=5000, dead_feature_threshold=1e-06, n_eval_batches=10, eval_batch_size_prompts=None, log_to_wandb=True, log_activations_store_to_wandb=False, log_optimizer_state_to_wandb=False, wandb_project='gemma_2b_exploration', wandb_id=None, run_name='16384-L1-2-LR-5e-05-Tokens-1.229e+09', wandb_entity=None, wandb_log_frequency=50, eval_every_n_wandb_logs=10, resume=False, n_checkpoints=5, checkpoint_path='checkpoints/adk2ig0r/01exu18z', verbose=True, model_kwargs={}, model_from_pretrained_kwargs={}, sae_lens_version='3.5.0', sae_lens_training_version='3.5.0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bd09526-8274-4e15-a062-d00153eb23ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architecture': 'jumprelu',\n",
       " 'd_in': 2304,\n",
       " 'd_sae': 16384,\n",
       " 'dtype': 'float32',\n",
       " 'model_name': 'gemma-2-2b',\n",
       " 'hook_name': 'blocks.5.hook_resid_post',\n",
       " 'hook_layer': 5,\n",
       " 'hook_head_index': None,\n",
       " 'activation_fn_str': 'relu',\n",
       " 'finetuning_scaling_factor': False,\n",
       " 'sae_lens_training_version': None,\n",
       " 'prepend_bos': True,\n",
       " 'dataset_path': 'monology/pile-uncopyrighted',\n",
       " 'context_size': 1024,\n",
       " 'dataset_trust_remote_code': True,\n",
       " 'apply_b_dec_to_input': False,\n",
       " 'normalize_activations': None,\n",
       " 'device': 'cpu'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_gemma_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "967bbc1d-f2c2-4de0-82b7-868a9472e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_name = 'gemma-2-2b-it'\n",
    "hook_layer = 5\n",
    "cfg.hook_layer = hook_layer\n",
    "cfg.hook_name = f\"blocks.{hook_layer}.hook_resid_post\"\n",
    "\n",
    "cfg.d_in = cfg_gemma_scope[\"d_in\"]\n",
    "cfg.d_sae = 16384\n",
    "\n",
    "cfg.dataset_path= cfg_gemma_scope[\"dataset_path\"]\n",
    "# cfg.dataset_path='lmsys/lmsys-chat-1m'\n",
    "cfg.streaming = True\n",
    "\n",
    "## for architecture\n",
    "cfg.architecture = cfg_gemma_scope[\"architecture\"]\n",
    "cfg.apply_b_dec_to_input=cfg_gemma_scope[\"apply_b_dec_to_input\"] ## let's check if this is true for gated models\n",
    "\n",
    "# Logging / evals\n",
    "cfg.log_to_wandb=True  # always use wandb unless you are just testing code.\n",
    "cfg.wandb_project=f\"{cfg.model_name}-SAEs-trial\"\n",
    "cfg.wandb_log_frequency=30\n",
    "cfg.eval_every_n_wandb_logs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8cd3e88-c9c9-41e1-a41a-d11460ffe64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163840000\n"
     ]
    }
   ],
   "source": [
    "divide_batch_size = 1\n",
    "\n",
    "total_training_steps = 40_000  # probably we should do more\n",
    "batch_size = int(4096 / divide_batch_size)\n",
    "total_training_tokens = total_training_steps * batch_size * divide_batch_size\n",
    "\n",
    "lr_warm_up_steps = l1_warm_up_steps = total_training_steps // 10  # 10% of training\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "\n",
    "print(total_training_tokens)\n",
    "\n",
    "cfg.training_tokens = total_training_tokens\n",
    "cfg.train_batch_size_tokens = batch_size\n",
    "cfg.lr_warm_up_steps = lr_warm_up_steps\n",
    "cfg.lr_decay_steps = lr_decay_steps\n",
    "cfg.store_batch_size_prompts = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "841908e3-2831-4e8c-b143-6e9c73180b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModelSAERunnerConfig(model_name='gemma-2-2b-it', model_class_name='HookedTransformer', hook_name='blocks.5.hook_resid_post', hook_eval='NOT_IN_USE', hook_layer=5, hook_head_index=None, dataset_path='monology/pile-uncopyrighted', dataset_trust_remote_code=True, streaming=True, is_dataset_tokenized=True, context_size=1024, use_cached_activations=False, cached_activations_path=None, architecture='jumprelu', d_in=2304, d_sae=16384, b_dec_init_method='zeros', expansion_factor=None, activation_fn='relu', activation_fn_kwargs={}, normalize_sae_decoder=False, noise_scale=0.0, from_pretrained_path=None, apply_b_dec_to_input=False, decoder_orthogonal_init=False, decoder_heuristic_init=True, init_encoder_as_decoder_transpose=True, n_batches_in_buffer=16, training_tokens=163840000, finetuning_tokens=0, store_batch_size_prompts=6, train_batch_size_tokens=4096, normalize_activations='none', seqpos_slice=(None,), device='cuda', act_store_device='cuda', seed=42, dtype='float32', prepend_bos=True, autocast=False, autocast_lm=False, compile_llm=True, llm_compilation_mode='max-autotune', compile_sae=False, sae_compilation_mode=None, adam_beta1=0.9, adam_beta2=0.999, mse_loss_normalization=None, l1_coefficient=2, lp_norm=1.0, scale_sparsity_penalty_by_decoder_norm=True, l1_warm_up_steps=15000, lr=5e-05, lr_scheduler_name='constant', lr_warm_up_steps=4000, lr_end=5e-06, lr_decay_steps=8000, n_restart_cycles=1, finetuning_method=None, use_ghost_grads=False, feature_sampling_window=5000, dead_feature_window=5000, dead_feature_threshold=1e-06, n_eval_batches=10, eval_batch_size_prompts=None, log_to_wandb=True, log_activations_store_to_wandb=False, log_optimizer_state_to_wandb=False, wandb_project='gemma-2-2b-it-SAEs-trial', wandb_id=None, run_name='16384-L1-2-LR-5e-05-Tokens-1.229e+09', wandb_entity=None, wandb_log_frequency=30, eval_every_n_wandb_logs=20, resume=False, n_checkpoints=5, checkpoint_path='checkpoints/adk2ig0r/01exu18z', verbose=True, model_kwargs={}, model_from_pretrained_kwargs={}, sae_lens_version='3.5.0', sae_lens_training_version='3.5.0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05faaaf8-5ed1-406d-a060-7abc87952f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b-it into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/data2/hms/dbmi/sunyaev/lab/dlee/.cache/pypoetry/virtualenvs/refusal-direction-f5Ymycjl-py3.12/lib/python3.12/site-packages/sae_lens/training/activations_store.py:246: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdajale423\u001b[0m (\u001b[33mboston\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/n/data2/hms/dbmi/sunyaev/lab/dlee/ai_safety/refusal_direction/notebooks/wandb/run-20241020_032510-8gjx4fdr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/boston/gemma-2-2b-it-SAEs-trial/runs/8gjx4fdr' target=\"_blank\">16384-L1-2-LR-5e-05-Tokens-1.229e+09</a></strong> to <a href='https://wandb.ai/boston/gemma-2-2b-it-SAEs-trial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/boston/gemma-2-2b-it-SAEs-trial' target=\"_blank\">https://wandb.ai/boston/gemma-2-2b-it-SAEs-trial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/boston/gemma-2-2b-it-SAEs-trial/runs/8gjx4fdr' target=\"_blank\">https://wandb.ai/boston/gemma-2-2b-it-SAEs-trial/runs/8gjx4fdr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/data2/hms/dbmi/sunyaev/lab/dlee/.cache/pypoetry/virtualenvs/refusal-direction-f5Ymycjl-py3.12/lib/python3.12/site-packages/sae_lens/training/sae_trainer.py:123: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.cfg.autocast)\n",
      "3500| MSE Loss 331.244 | L1 704.431:   9%|████████▏                                                                                     | 14336000/163840000 [28:30<4:22:34, 9489.32it/s]"
     ]
    }
   ],
   "source": [
    "# print(\"Comment this code out to train! Otherwise, it will load in the already trained model.\")\n",
    "t.set_grad_enabled(True)\n",
    "runner = SAETrainingRunner(cfg)\n",
    "sae = runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eeac3a-02d5-4029-be54-85b855c41c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
