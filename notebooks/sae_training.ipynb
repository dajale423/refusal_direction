{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ea1f85-e78a-4e47-a690-e55f884187fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "395d84f5-2a27-4399-8978-44abbe2e80f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from sae_vis import SaeVisConfig, SaeVisData, SaeVisLayoutConfig\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19e6a8d-c8c2-4137-989f-78aae842bfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-1L-21M into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 64.66it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 156.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 153.89it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 150.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 163.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───┬──────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ 0 │ Once upon a time, there was a bag. He opened the box and there was a comfortable waffle. But it was  │\n",
      "│   │ very high, and the zebra found a very special and expensive toy. With the waffle, they could enjoy a │\n",
      "│   │ cozy show inside!                                                                                    │\n",
      "├───┼──────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ 1 │ Once upon a time there was a little girl named Zoey. Zoey loves to play snow. She likes to wear      │\n",
      "│   │ colorful scarves. She makes a special costume for her friends, and she really likes to wear them     │\n",
      "│   │ when they wear it. Zoey likes her costume                                                            │\n",
      "├───┼──────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ 2 │ Once upon a time, there were a fair princess who lived in a palace. She loved to play. Every night   │\n",
      "│   │ she went out to herself in the kingdom every night, surrounded by imagining things she's exploring   │\n",
      "│   │ the world around her - all she would dream about flying like big                                     │\n",
      "├───┼──────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ 3 │ Once upon a time, there was a jolly little bear named Skip. Skip loved to play and explore, but one  │\n",
      "│   │ day he was sad because he was always troubled. He thought   One day, he wanted to try something new. │\n",
      "│   │ He decided to try and                                                                                │\n",
      "├───┼──────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ 4 │ Once upon a time, she got an idea. She wanted to make a friend with. She found a pet frog and they   │\n",
      "│   │ both liked them.   The two friends had lots of fun together. They even brought lots of things, like  │\n",
      "│   │ carrots, apples, and                                                                                 │\n",
      "└───┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tinystories_model = HookedSAETransformer.from_pretrained(\"tiny-stories-1L-21M\")\n",
    "\n",
    "completions = [(i, tinystories_model.generate(\"Once upon a time\", temperature=1, max_new_tokens=50)) for i in range(5)]\n",
    "\n",
    "print(tabulate(completions, tablefmt=\"simple_grid\", maxcolwidths=[None, 100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00019c8-5ad9-4d81-bfaa-4317bed01482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 16384-L1-4-LR-5e-05-Tokens-1.229e+08\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 30000\n",
      "Total wandb updates: 1000\n",
      "n_tokens_per_feature_sampling_window (millions): 4194.304\n",
      "n_tokens_per_dead_feature_window (millions): 2097.152\n",
      "We will reset the sparsity calculation 15 times.\n",
      "Number tokens in sparsity calculation window: 8.19e+06\n",
      "Comment this code out to train! Otherwise, it will load in the already trained model.\n",
      "Loaded pretrained model tiny-stories-1L-21M into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 415/415 [00:00<00:00, 6.06kB/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdajale423\u001b[0m (\u001b[33mboston\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/n/data2/hms/dbmi/sunyaev/lab/dlee/ai_safety/refusal_direction/notebooks/wandb/run-20241017_184429-v0mb3005</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/boston/arena-demos-tinystories/runs/v0mb3005' target=\"_blank\">16384-L1-4-LR-5e-05-Tokens-1.229e+08</a></strong> to <a href='https://wandb.ai/boston/arena-demos-tinystories' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/boston/arena-demos-tinystories' target=\"_blank\">https://wandb.ai/boston/arena-demos-tinystories</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/boston/arena-demos-tinystories/runs/v0mb3005' target=\"_blank\">https://wandb.ai/boston/arena-demos-tinystories/runs/v0mb3005</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/data2/hms/dbmi/sunyaev/lab/dlee/.cache/pypoetry/virtualenvs/refusal-direction-f5Ymycjl-py3.12/lib/python3.12/site-packages/sae_lens/training/sae_trainer.py:123: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.cfg.autocast)\n",
      "4600| MSE Loss 49.299 | L1 72.607:  15%|██████████████▊                                                                                  | 18841600/122880000 [17:20<1:31:53, 18870.57it/s]"
     ]
    }
   ],
   "source": [
    "total_training_steps = 30_000  # probably we should do more\n",
    "batch_size = 4096\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = l1_warm_up_steps = total_training_steps // 10  # 10% of training\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    #\n",
    "    # Data generation\n",
    "    model_name=\"tiny-stories-1L-21M\",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n",
    "    hook_name=\"blocks.0.hook_mlp_out\",\n",
    "    hook_layer=0,\n",
    "    d_in=tinystories_model.cfg.d_model,\n",
    "    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",  # tokenized language dataset on HF for the Tiny Stories corpus.\n",
    "    is_dataset_tokenized=True,\n",
    "    prepend_bos=True,  # you should use whatever the base model was trained with\n",
    "    streaming=True,  # we could pre-download the token dataset if it was small.\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    context_size=512,  # larger is better but takes longer (for tutorial we'll use a short one)\n",
    "    #\n",
    "    # SAE architecture\n",
    "    architecture=\"gated\",\n",
    "    expansion_factor=16,\n",
    "    b_dec_init_method=\"zeros\",\n",
    "    apply_b_dec_to_input=True,\n",
    "    normalize_sae_decoder=False,\n",
    "    scale_sparsity_penalty_by_decoder_norm=True,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    #\n",
    "    # Activations store\n",
    "    n_batches_in_buffer=64,\n",
    "    training_tokens=total_training_tokens,\n",
    "    store_batch_size_prompts=16,\n",
    "    #\n",
    "    # Training hyperparameters (standard)\n",
    "    lr=5e-5,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",  # controls how the LR warmup / decay works\n",
    "    lr_warm_up_steps=lr_warm_up_steps,  # avoids large number of initial dead features\n",
    "    lr_decay_steps=lr_decay_steps,  # helps avoid overfitting\n",
    "    #\n",
    "    # Training hyperparameters (SAE-specific)\n",
    "    l1_coefficient=4,\n",
    "    l1_warm_up_steps=l1_warm_up_steps,\n",
    "    use_ghost_grads=False,  # we don't use ghost grads anymore\n",
    "    feature_sampling_window=2000,  # how often we resample dead features\n",
    "    dead_feature_window=1000,  # size of window to assess whether a feature is dead\n",
    "    dead_feature_threshold=1e-4,  # threshold for classifying feature as dead, over window\n",
    "    #\n",
    "    # Logging / evals\n",
    "    log_to_wandb=True,  # always use wandb unless you are just testing code.\n",
    "    wandb_project=\"arena-demos-tinystories\",\n",
    "    wandb_log_frequency=30,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "    #\n",
    "    # Misc.\n",
    "    device=str(device),\n",
    "    seed=42,\n",
    "    n_checkpoints=5,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "print(\"Comment this code out to train! Otherwise, it will load in the already trained model.\")\n",
    "t.set_grad_enabled(True)\n",
    "runner = SAETrainingRunner(cfg)\n",
    "sae = runner.run()\n",
    "\n",
    "hf_repo_id = \"callummcdougall/arena-demos-tinystories\"\n",
    "sae_id = cfg.hook_name\n",
    "\n",
    "# upload_saes_to_huggingface({sae_id: sae}, hf_repo_id=hf_repo_id)\n",
    "\n",
    "tinystories_sae = SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d181df64-df63-40fb-b96e-245b7736b843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 512])\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(cfg.dataset_path, streaming=True)\n",
    "batch_size = 1024\n",
    "tokens = t.tensor(\n",
    "    [x[\"input_ids\"] for i, x in zip(range(batch_size), dataset[\"train\"])],\n",
    "    device=str(device),\n",
    ")\n",
    "print(tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e03e26b0-d138-40c2-9991-941d9ac8202e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SaeVisData.create() got an unexpected keyword argument 'sae'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sae_vis_data \u001b[38;5;241m=\u001b[39m \u001b[43mSaeVisData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43msae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtinystories_sae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtinystories_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSaeVisConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m sae_vis_data\u001b[38;5;241m.\u001b[39msave_feature_centric_vis(\n\u001b[1;32m      9\u001b[0m     filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(section_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_vis.html\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     10\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# If this display code doesn't work, you might need to download the file & open in browser to see it\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: SaeVisData.create() got an unexpected keyword argument 'sae'"
     ]
    }
   ],
   "source": [
    "sae_vis_data = SaeVisData.create(\n",
    "    sae=tinystories_sae,\n",
    "    model=tinystories_model,\n",
    "    tokens=tokens,\n",
    "    cfg=SaeVisConfig(features=range(16)),\n",
    "    verbose=True,\n",
    ")\n",
    "sae_vis_data.save_feature_centric_vis(\n",
    "    filename=str(section_dir / \"feature_vis.html\"),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# If this display code doesn't work, you might need to download the file & open in browser to see it\n",
    "with open(str(section_dir / \"feature_vis.html\")) as f:\n",
    "    display(HTML(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62f56df1-ab67-4442-9a4f-54a138223de3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "sae, cfg, sparsity  = SAE.from_pretrained(\n",
    "  \"gemma-2b-it-res-jb\", # to see the list of available releases, go to: https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n",
    "  \"blocks.12.hook_resid_post\" # change this to another specific SAE ID in the release if desired. \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1c70348-0bbb-44c4-af1d-d095df846718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'gemma-2b-it',\n",
       " 'model_class_name': 'HookedTransformer',\n",
       " 'hook_name': 'blocks.12.hook_resid_post',\n",
       " 'hook_eval': 'NOT_IN_USE',\n",
       " 'hook_layer': 12,\n",
       " 'hook_head_index': None,\n",
       " 'dataset_path': 'Skylion007/openwebtext',\n",
       " 'dataset_trust_remote_code': True,\n",
       " 'streaming': False,\n",
       " 'is_dataset_tokenized': True,\n",
       " 'context_size': 1024,\n",
       " 'use_cached_activations': False,\n",
       " 'cached_activations_path': None,\n",
       " 'd_in': 2048,\n",
       " 'd_sae': 16384,\n",
       " 'b_dec_init_method': 'zeros',\n",
       " 'expansion_factor': 8,\n",
       " 'activation_fn': 'relu',\n",
       " 'normalize_sae_decoder': False,\n",
       " 'noise_scale': 0.0,\n",
       " 'from_pretrained_path': None,\n",
       " 'apply_b_dec_to_input': False,\n",
       " 'decoder_orthogonal_init': False,\n",
       " 'decoder_heuristic_init': True,\n",
       " 'init_encoder_as_decoder_transpose': True,\n",
       " 'n_batches_in_buffer': 16,\n",
       " 'training_tokens': 1228800000,\n",
       " 'finetuning_tokens': 0,\n",
       " 'store_batch_size_prompts': 8,\n",
       " 'train_batch_size_tokens': 4096,\n",
       " 'normalize_activations': 'none',\n",
       " 'device': 'cpu',\n",
       " 'act_store_device': 'cuda',\n",
       " 'seed': 42,\n",
       " 'dtype': 'float32',\n",
       " 'prepend_bos': True,\n",
       " 'autocast': False,\n",
       " 'autocast_lm': False,\n",
       " 'compile_llm': True,\n",
       " 'llm_compilation_mode': 'max-autotune',\n",
       " 'compile_sae': False,\n",
       " 'sae_compilation_mode': None,\n",
       " 'adam_beta1': 0.9,\n",
       " 'adam_beta2': 0.999,\n",
       " 'mse_loss_normalization': None,\n",
       " 'l1_coefficient': 2,\n",
       " 'lp_norm': 1.0,\n",
       " 'scale_sparsity_penalty_by_decoder_norm': True,\n",
       " 'l1_warm_up_steps': 15000,\n",
       " 'lr': 5e-05,\n",
       " 'lr_scheduler_name': 'constant',\n",
       " 'lr_warm_up_steps': 0,\n",
       " 'lr_end': 5e-06,\n",
       " 'lr_decay_steps': 60000,\n",
       " 'n_restart_cycles': 1,\n",
       " 'finetuning_method': None,\n",
       " 'use_ghost_grads': False,\n",
       " 'feature_sampling_window': 5000,\n",
       " 'dead_feature_window': 5000,\n",
       " 'dead_feature_threshold': 1e-06,\n",
       " 'n_eval_batches': 10,\n",
       " 'eval_batch_size_prompts': None,\n",
       " 'log_to_wandb': True,\n",
       " 'log_activations_store_to_wandb': False,\n",
       " 'log_optimizer_state_to_wandb': False,\n",
       " 'wandb_project': 'gemma_2b_exploration',\n",
       " 'wandb_id': None,\n",
       " 'run_name': '16384-L1-2-LR-5e-05-Tokens-1.229e+09',\n",
       " 'wandb_entity': None,\n",
       " 'wandb_log_frequency': 50,\n",
       " 'eval_every_n_wandb_logs': 10,\n",
       " 'resume': False,\n",
       " 'n_checkpoints': 5,\n",
       " 'checkpoint_path': 'checkpoints/adk2ig0r',\n",
       " 'verbose': True,\n",
       " 'model_kwargs': {},\n",
       " 'model_from_pretrained_kwargs': {},\n",
       " 'sae_lens_version': '3.5.0',\n",
       " 'sae_lens_training_version': '3.5.0',\n",
       " 'tokens_per_buffer': 67108864,\n",
       " 'finetuning_scaling_factor': False,\n",
       " 'activation_fn_str': 'relu',\n",
       " 'architecture': 'standard',\n",
       " 'neuronpedia': None}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dea67e76-8315-421b-8133-304aaaf3c7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architecture': 'jumprelu',\n",
       " 'd_in': 2304,\n",
       " 'd_sae': 16384,\n",
       " 'dtype': 'float32',\n",
       " 'model_name': 'gemma-2-2b',\n",
       " 'hook_name': 'blocks.5.hook_resid_post',\n",
       " 'hook_layer': 5,\n",
       " 'hook_head_index': None,\n",
       " 'activation_fn_str': 'relu',\n",
       " 'finetuning_scaling_factor': False,\n",
       " 'sae_lens_training_version': None,\n",
       " 'prepend_bos': True,\n",
       " 'dataset_path': 'monology/pile-uncopyrighted',\n",
       " 'context_size': 1024,\n",
       " 'dataset_trust_remote_code': True,\n",
       " 'apply_b_dec_to_input': False,\n",
       " 'normalize_activations': None,\n",
       " 'device': 'cpu'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "sae, cfg, sparsity  = SAE.from_pretrained(\n",
    "  release = \"gemma-scope-2b-pt-res-canonical\", # to see the list of available releases, go to: https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n",
    "  sae_id = \"layer_5/width_16k/canonical\" # change this to another specific SAE ID in the release if desired. \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d04a7daf-e150-4728-a43b-4bd9d933a752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.47s/it]\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b-it into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gemma_2b_model = HookedSAETransformer.from_pretrained(\"gemma-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49763fa4-d2a6-4b92-8e64-d9b3a51be0f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 2048-L1-0.001-LR-0.0003-Tokens-2.000e+06\n",
      "n_tokens_per_buffer (millions): 0.08192\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "Total training steps: 488\n",
      "Total wandb updates: 48\n",
      "n_tokens_per_feature_sampling_window (millions): 1048.576\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 0 times.\n",
      "Number tokens in sparsity calculation window: 8.19e+06\n"
     ]
    }
   ],
   "source": [
    "new_cfg = LanguageModelSAERunnerConfig(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db09dd96-803d-4823-ac69-5eebd9e103d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModelSAERunnerConfig(model_name='gemma-2b-it', model_class_name='HookedTransformer', hook_name='blocks.5.hook_resid_post', hook_eval='NOT_IN_USE', hook_layer=5, hook_head_index=None, dataset_path='Skylion007/openwebtext', dataset_trust_remote_code=True, streaming=True, is_dataset_tokenized=True, context_size=1024, use_cached_activations=False, cached_activations_path=None, architecture='jumprelu', d_in=2048, d_sae=16384, b_dec_init_method='zeros', expansion_factor=8, activation_fn='relu', activation_fn_kwargs={}, normalize_sae_decoder=False, noise_scale=0.0, from_pretrained_path=None, apply_b_dec_to_input=False, decoder_orthogonal_init=False, decoder_heuristic_init=True, init_encoder_as_decoder_transpose=True, n_batches_in_buffer=64, training_tokens=122880000, finetuning_tokens=0, store_batch_size_prompts=16, train_batch_size_tokens=4096, normalize_activations='none', seqpos_slice=(None,), device='cuda', act_store_device='cuda', seed=42, dtype='float32', prepend_bos=True, autocast=False, autocast_lm=False, compile_llm=False, llm_compilation_mode=None, compile_sae=False, sae_compilation_mode=None, adam_beta1=0.9, adam_beta2=0.999, mse_loss_normalization=None, l1_coefficient=4, lp_norm=1, scale_sparsity_penalty_by_decoder_norm=True, l1_warm_up_steps=3000, lr=5e-05, lr_scheduler_name='constant', lr_warm_up_steps=3000, lr_end=5e-06, lr_decay_steps=6000, n_restart_cycles=1, finetuning_method=None, use_ghost_grads=False, feature_sampling_window=2000, dead_feature_window=1000, dead_feature_threshold=0.0001, n_eval_batches=10, eval_batch_size_prompts=None, log_to_wandb=True, log_activations_store_to_wandb=False, log_optimizer_state_to_wandb=False, wandb_project='arena-demos-tinystories', wandb_id=None, run_name='16384-L1-4-LR-5e-05-Tokens-1.229e+08', wandb_entity=None, wandb_log_frequency=30, eval_every_n_wandb_logs=20, resume=False, n_checkpoints=5, checkpoint_path='checkpoints/00uh57fd', verbose=True, model_kwargs={}, model_from_pretrained_kwargs={'center_writing_weights': False}, sae_lens_version='4.0.0', sae_lens_training_version='4.0.0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bce99f2-dfef-46ea-983b-8d0e7c90df07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 16384-L1-4-LR-5e-05-Tokens-1.229e+08\n",
      "n_tokens_per_buffer (millions): 1.048576\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 30000\n",
      "Total wandb updates: 1000\n",
      "n_tokens_per_feature_sampling_window (millions): 8388.608\n",
      "n_tokens_per_dead_feature_window (millions): 4194.304\n",
      "We will reset the sparsity calculation 15 times.\n",
      "Number tokens in sparsity calculation window: 8.19e+06\n"
     ]
    }
   ],
   "source": [
    "total_training_steps = 30_000  # probably we should do more\n",
    "batch_size = 4096\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = l1_warm_up_steps = total_training_steps // 10  # 10% of training\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "\n",
    "old_cfg = LanguageModelSAERunnerConfig(\n",
    "    #\n",
    "    # Data generation\n",
    "    model_name=\"gemma-2b-it\",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n",
    "    hook_name=\"blocks.5.hook_resid_post\",\n",
    "    hook_layer=5,\n",
    "    d_in=gemma_2b_model.cfg.d_model,\n",
    "    dataset_path='monology/pile-uncopyrighted',\n",
    "    is_dataset_tokenized=True,\n",
    "    prepend_bos=True,  # you should use whatever the base model was trained with\n",
    "    streaming=True,  # we could pre-download the token dataset if it was small.\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    context_size=1024,  # larger is better but takes longer (for tutorial we'll use a short one)\n",
    "    #\n",
    "    # SAE architecture\n",
    "    architecture=\"gated\",\n",
    "    expansion_factor=8,\n",
    "    b_dec_init_method=\"zeros\",\n",
    "    apply_b_dec_to_input=False,\n",
    "    normalize_sae_decoder=False,\n",
    "    scale_sparsity_penalty_by_decoder_norm=True,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    #\n",
    "    # Activations store\n",
    "    n_batches_in_buffer=64,\n",
    "    training_tokens=total_training_tokens,\n",
    "    store_batch_size_prompts=16,\n",
    "    #\n",
    "    # Training hyperparameters (standard)\n",
    "    lr=5e-5,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",  # controls how the LR warmup / decay works\n",
    "    lr_warm_up_steps=lr_warm_up_steps,  # avoids large number of initial dead features\n",
    "    lr_decay_steps=lr_decay_steps,  # helps avoid overfitting\n",
    "    #\n",
    "    # Training hyperparameters (SAE-specific)\n",
    "    l1_coefficient=4,\n",
    "    l1_warm_up_steps=l1_warm_up_steps,\n",
    "    use_ghost_grads=False,  # we don't use ghost grads anymore\n",
    "    feature_sampling_window=2000,  # how often we resample dead features\n",
    "    dead_feature_window=1000,  # size of window to assess whether a feature is dead\n",
    "    dead_feature_threshold=1e-4,  # threshold for classifying feature as dead, over window\n",
    "    #\n",
    "    # Logging / evals\n",
    "    log_to_wandb=True,  # always use wandb unless you are just testing code.\n",
    "    wandb_project=\"arena-demos-tinystories\",\n",
    "    wandb_log_frequency=30,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "    #\n",
    "    # Misc.\n",
    "    device=str(device),\n",
    "    seed=42,\n",
    "    n_checkpoints=5,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "# print(\"Comment this code out to train! Otherwise, it will load in the already trained model.\")\n",
    "# t.set_grad_enabled(True)\n",
    "# runner = SAETrainingRunner(cfg)\n",
    "# sae = runner.run()\n",
    "\n",
    "# hf_repo_id = \"callummcdougall/arena-demos-tinystories\"\n",
    "# sae_id = cfg.hook_name\n",
    "\n",
    "# # upload_saes_to_huggingface({sae_id: sae}, hf_repo_id=hf_repo_id)\n",
    "\n",
    "# tinystories_sae = SAE.from_pretrained(release=hf_repo_id, sae_id=sae_id, device=str(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05faaaf8-5ed1-406d-a060-7abc87952f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
