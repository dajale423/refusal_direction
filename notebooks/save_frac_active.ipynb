{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/poesy/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Literal, TypeAlias\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import torch as t\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "from jaxtyping import Float, Int\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from sae_vis import SaeVisConfig, SaeVisData, SaeVisLayoutConfig\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "device = \"cuda\" if t.cuda.is_available() else \"mps\" if t.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 45589.06 MB\n",
      "Allocated GPU Memory: 0.00 MB\n",
      "Cached GPU Memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "## check memory usage\n",
    "\n",
    "if t.cuda.is_available():\n",
    "    gpu_id = 0  # Set to your target GPU ID\n",
    "    total_memory = t.cuda.get_device_properties(gpu_id).total_memory\n",
    "    allocated_memory = t.cuda.memory_allocated(gpu_id)\n",
    "    cached_memory = t.cuda.memory_reserved(gpu_id)\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory / 1024**2:.2f} MB\")\n",
    "    print(f\"Allocated GPU Memory: {allocated_memory / 1024**2:.2f} MB\")\n",
    "    print(f\"Cached GPU Memory: {cached_memory / 1024**2:.2f} MB\")\n",
    "elif t.backends.mps.is_available():\n",
    "    # MPS (Metal Performance Shaders) for Mac\n",
    "    print(\"MPS is available.\")\n",
    "    # Note: As of now, PyTorch doesn't provide direct memory management functions for MPS\n",
    "    print(\"Memory information is not available for MPS.\")\n",
    "else:\n",
    "    print(\"Neither CUDA nor MPS is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del gemma2\n",
    "# del gemma2_sae\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "t.cuda.empty_cache()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output tensor of SAE activations for advbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31323\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read from advbench.json file\n",
    "with open('/workspace/refusal_direction/dataset/processed/advbench.json', 'r') as file:\n",
    "    advbench_data = json.load(file)\n",
    "\n",
    "len(advbench_data)\n",
    "\n",
    "# Read from advbench.json file\n",
    "with open('/workspace/refusal_direction/dataset/processed/alpaca.json', 'r') as file:\n",
    "    alpaca_data = json.load(file)\n",
    "\n",
    "print(len(alpaca_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sae_activations(sae_name, sae_ids, data, suffix, max_data_rows=None):\n",
    "    t.set_grad_enabled(False)\n",
    "    gemma2: HookedSAETransformer = HookedSAETransformer.from_pretrained(\"gemma-2-2b-it\", device=device)\n",
    "\n",
    "    for sae_id in sae_ids:\n",
    "        print(f\"Calculating activation fraction for {sae_id}\")\n",
    "        gemma2_sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "            release=sae_name,\n",
    "            sae_id=sae_id,\n",
    "            device=str(device),\n",
    "        )\n",
    "    \n",
    "        sum_sae_nonzero_acts_post_last = t.zeros(gemma2_sae.cfg.d_sae).to(device)\n",
    "        sum_sae_nonzero_acts_post_all = t.zeros(gemma2_sae.cfg.d_sae).to(device)\n",
    "        last_token_count = 0\n",
    "        all_token_count = 0\n",
    "    \n",
    "        for item in tqdm(data[:max_data_rows]):\n",
    "            prompt = item['instruction']\n",
    "            \n",
    "            # Get top activations on final token\n",
    "            _, cache = gemma2.run_with_cache_with_saes(\n",
    "                prompt,\n",
    "                saes=[gemma2_sae],\n",
    "                stop_at_layer=gemma2_sae.cfg.hook_layer + 1,\n",
    "            )\n",
    "            cache_post = cache[f\"{gemma2_sae.cfg.hook_name}.hook_sae_acts_post\"]\n",
    "\n",
    "            last_token_count += 1\n",
    "            sum_sae_nonzero_acts_post_last += (cache_post[0, -1, :] > 0)\n",
    "\n",
    "            all_token_count += cache_post.shape[1]\n",
    "            sum_sae_nonzero_acts_post_all += (cache_post[0, :, :] > 0).sum(dim=0)\n",
    "    \n",
    "        # Stack all sae_acts_post tensors\n",
    "        frac_active_last = sum_sae_nonzero_acts_post_last / last_token_count\n",
    "        frac_active_all = sum_sae_nonzero_acts_post_all / all_token_count\n",
    "    \n",
    "        print(f\"SAE ID: {sae_id}\")\n",
    "        print(f\"{last_token_count=}\")\n",
    "        print(f\"{all_token_count=}\")\n",
    "        print(f\"Shape of frac_active_last tensor: {frac_active_last.shape}\")\n",
    "        print(f\"Shape of frac_active_all tensor: {frac_active_all.shape}\")\n",
    "        print(f\"Total number of non-zero activations at last token: {(sum_sae_nonzero_acts_post_last != 0).sum().item()}\")\n",
    "        print(f\"Total number of non-zero activations at all tokens: {(sum_sae_nonzero_acts_post_all != 0).sum().item()}\")\n",
    "    \n",
    "        # Create directory if it doesn't exist\n",
    "        directory = f'/workspace/refusal_direction/data/sae_frac_active/{sae_name}/{sae_id}'\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "        # Save the stacked_sae_acts_post tensor\n",
    "        t.save(frac_active_last.cpu(), f'{directory}_{suffix}_{last_token_count}_last.pt')\n",
    "        t.save(frac_active_all.cpu(), f'{directory}_{suffix}_{all_token_count}_all.pt')\n",
    "    \n",
    "        # Print confirmation message\n",
    "        print(f\"Fractions of SAEs active saved to '{directory}'\")\n",
    "        print(\"---\")\n",
    "    \n",
    "        del sum_sae_nonzero_acts_post_last,frac_active_last,sum_sae_nonzero_acts_post_all,frac_active_all\n",
    "        del gemma2_sae\n",
    "        gc.collect()\n",
    "        t.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b-it into HookedTransformer\n",
      "Calculating activation fraction for layer_0/width_16k/canonical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [07:52<00:00, 21.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE ID: layer_0/width_16k/canonical\n",
      "last_token_count=10000\n",
      "all_token_count=163840000\n",
      "Shape of frac_active_last tensor: torch.Size([16384])\n",
      "Shape of frac_active_all tensor: torch.Size([16384])\n",
      "Total number of non-zero activations at last token: 15010\n",
      "Total number of non-zero activations at all tokens: 16379\n",
      "Fractions of SAEs active saved to '/workspace/refusal_direction/data/sae_frac_active/gemma-scope-2b-pt-res-canonical/layer_0/width_16k/canonical'\n",
      "---\n",
      "Calculating activation fraction for layer_1/width_16k/canonical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5024/10000 [04:19<04:08, 20.01it/s]"
     ]
    }
   ],
   "source": [
    "sae_name = \"gemma-scope-2b-pt-res-canonical\"\n",
    "sae_ids = [f\"layer_{layer}/width_16k/canonical\" for layer in range(11)]  # 0 through 10\n",
    "#sae_ids = [f\"layer_{layer}/width_16k/canonical\" for layer in [5]]  # only layer 5\n",
    "\n",
    "save_sae_activations(sae_name, sae_ids, alpaca_data, suffix = \"alpaca\", max_data_rows=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poesy",
   "language": "python",
   "name": "poesy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
